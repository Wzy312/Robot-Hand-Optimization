
sac_pso_batch = {
    'name': 'Experiment 1: PSO Batch',
    'data_folder': 'data_exp_sac_pso_batch',
    'iterations_init': 100,
    'iterations_random': 50,
    'iterations': 1000,
    'design_cycles': 50,
    'state_batch_size': 64,
    'initial_episodes': 10,
    'use_gpu': True,
    'cuda_device': 0,
    'exploration_strategy': 'random',
    'design_optim_method': 'pso_batch',
    'steps_per_episodes': 1000,
    'save_networks': True,
    'rl_method': 'SoftActorCritic',
    'rl_algorithm_config': dict(
        algo_params=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.005,
            target_update_period=1,
            policy_lr=3E-4,
            qf_lr=3E-4,
            alpha=0.01,
        ),
        algo_params_pop=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.005,
            target_update_period=1,
            policy_lr=3E-4,
            qf_lr=3E-4,
            alpha=0.01,
        ),
        net_size=200,
        network_depth=3,
        copy_from_gobal=True,
        indiv_updates=1000,
        pop_updates=250,
        batch_size=256,
    ),
    'env': dict(
        env_name='HalfCheetah',
        render=False,
        record_video=False,
    ),
}

# AllegroHand
allegrohand_pso_batch = {
    'name': 'Allegro Hand PSO Batch',
    'data_folder': 'data_exp_allegro_pso_batch',
    'iterations_init': 50,
    'iterations_random': 50,
    'iterations': 50,
    'design_cycles': 50,
    'state_batch_size': 64,
    'initial_episodes': 5,
    'use_gpu': True,
    'cuda_device': 0,
    'exploration_strategy': 'random',
    'design_optim_method': 'pso_batch',
    'steps_per_episodes': 1000,
    'save_networks': True,
    'rl_method': 'SoftActorCritic',
    'rl_algorithm_config': dict(
        algo_params=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.001,
            target_update_period=1,
            policy_lr=1E-4,
            qf_lr=1E-4,
            alpha=0.02,
        ),
        algo_params_pop=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.005,
            target_update_period=1,
            policy_lr=1E-4,
            qf_lr=1E-4,
            alpha=0.01,
        ),
        net_size=256,
        network_depth=3,
        copy_from_gobal=True,
        indiv_updates=1000,
        pop_updates=500,
        batch_size=256,
    ),
    'env': dict(
        env_name='AllegroHand',
        render=True,
        record_video=False,
    ),
}


sac_pso_sim = {
    'name': 'Experiment 2: PSO using Simulations',
    'data_folder': 'data_exp_sac_pso_sim',
    'iterations_init': 200,
    'iterations': 200,
    'design_cycles': 50,
    'state_batch_size': 32,
    'initial_episodes': 10,
    'use_gpu': True,
    'cuda_device': 0,
    'exploration_strategy': 'random',
    'design_optim_method': 'pso_sim',
    'save_networks': True,
    'rl_method': 'SoftActorCritic',
    'rl_algorithm_config': dict(
        algo_params=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.005,
            target_update_period=1,
            policy_lr=3E-4,
            qf_lr=3E-4,
            alpha=0.01,
        ),
        algo_params_pop=dict(
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=0.005,
            target_update_period=1,
            policy_lr=3E-4,
            qf_lr=3E-4,
            alpha=0.01,
        ),
        net_size=512,
        network_depth=3,
        copy_from_gobal=True,
        indiv_updates=1000,
        pop_updates=250,
        batch_size=256,
    ),
    'env': dict(
        env_name='HalfCheetah',
        render=True,
        record_video=False,
    ),
}


config_dict = {
    'base': sac_pso_batch,
    'sac_pso_batch': sac_pso_batch,
    'sac_pso_sim': sac_pso_sim,
    'allegrohand': allegrohand_pso_batch
}
